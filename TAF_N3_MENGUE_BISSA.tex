\documentclass[12pt,a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french,english]{babel}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{pgfplots}
\usetikzlibrary{calc}
\usepackage{geometry}
\usepackage{array}
\usepackage{xcolor}
\usepackage{fancyhdr}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{geometry}
\geometry{margin=2.3cm}
\usepackage{amsmath,amssymb}
\usepackage{array}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{tikz}
\usetikzlibrary{positioning,shapes,arrows.meta}
\usepackage{float}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{tabularx}


\geometry{a4paper, margin=1.8cm}

\fancyhead[L]{Investigation Numérique}
\fancyhead[R]{\textit{MENGUE BISSA}}
\renewcommand{\headrulewidth}{0.5pt}

\geometry{left=18mm,right=18mm,top=18mm,bottom=18mm}

\begin{document}
	\thispagestyle{empty}
	
	
	\begin{tikzpicture}[remember picture,overlay]
		\draw[blue,line width=6pt]
		($(current page.north west)+(1.2cm,-1.2cm)$) rectangle
		($(current page.south east)+(-1.2cm,1.2cm)$);
		\draw[black,line width=0.5pt]
		($(current page.north west)+(1.4cm,-1.4cm)$) rectangle
		($(current page.south east)+(-1.4cm,1.4cm)$);
	\end{tikzpicture}
	
	
	\vspace*{-1cm} 
	\begin{center}
		\begin{tabular}{m{0.40\textwidth} m{0.15\textwidth} m{0.40\textwidth}}
		
			\centering\small
			RÉPUBLIQUE DU CAMEROUN\\
			\textit{Paix -- Travail -- Patrie}\\
			******\\
			MINISTÈRE DE L'ENSEIGNEMENT SUPÉRIEUR\\
			******\\
			\textbf{UNIVERSITÉ DE YAOUNDÉ I}\\
			******\\
			\textbf{ÉCOLE NATIONALE SUPÉRIEURE POLYTECHNIQUE}\\
			******\\
			DÉPARTEMENT DE GÉNIE INFORMATIQUE\\
            ******\\
			&
		
			\centering
			\raisebox{0.5cm}{\includegraphics[height=3cm]{ENSPY.jpg}} 
			&
			
			\centering\small
			\hspace{0.2cm} 
			REPUBLIC OF CAMEROON\\
			\textit{Peace -- Work -- Fatherland}\\
			******\\
			MINISTRY OF HIGHER \\
            EDUCATION\\
			******\\
			\textbf{UNIVERSITY OF YAOUNDE I}\\
			******\\
			\textbf{NATIONAL ADVANCED SCHOOL OF ENGINEERING}\\
			******\\
			COMPUTER ENGINEERING DEPARTMENT\\
            ******\\
		\end{tabular}
	\end{center}
	

	\vspace{0.5cm}
	\begin{center}
		\fcolorbox{black}{white}{%
			\parbox{0.85\textwidth}{%
				\vspace{0.4cm}
				\centering
				\bfseries\fontsize{21pt}{23pt}\selectfont
				INTRODUCTION AUX TECHNIQUES\\
				D'INVESTIGATION NUMÉRIQUE
				\vspace{0.4cm}
		}}
	\end{center}
	

	\vspace{0.8cm}
	\begin{center}
		\fcolorbox{black}{blue!40!blue}{%
			\parbox{0.8\textwidth}{%
				\vspace{0.5cm}
				\centering
				\color{white}\bfseries\fontsize{22pt}{28pt}\selectfont
				EXERÇONS-NOUS CHAPITRE 2
				\vspace{0.5cm}
		}}
	\end{center}
	
	
	\vspace{1.5cm}
	\begin{center}
			\fontsize{15pt}{20pt}\selectfont
            \textit{Rédigé par}\\
            \textbf{MENGUE BISSA MARGUERITE}\\
            \textit{Matricule : \textbf{22P064}}
	\end{center}
	
	\vspace{1.5cm}
	\begin{center}
			\fontsize{15pt}{20pt}\selectfont
            \textit{Sous la supervision de}\\
            \textbf{Mr Thierry MINKA}\\
	\end{center}
	

	\vspace{1cm}
	\begin{center}
		\fcolorbox{black}{blue!10!blue}{%
			\parbox{0.4\textwidth}{%
				\vspace{0.3cm}
				\centering
				\color{white}\bfseries\fontsize{10pt}{10pt}\selectfont
				Année académique 2025/2026
				\vspace{0.2cm}
		}}
	\end{center}
	
	\vfill
	\pagestyle{fancy}
    \newpage
    \pagestyle{fancy}
	
	{\Large
		
		{\Large

}

\begin{document}

\section*{Partie 1 : Analyse Historique et Épistémologique}

\subsection*{1. Analyse comparative des régimes de vérité}\
\subsection*{a)Choix des deux périodes}
\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{8pt}
\begin{tabular}{|c|p{10cm}|}
\hline
\textbf{Période} & \textbf{Caractéristiques générales} \\
\hline
1990--2000 (Ère de la professionnalisation) &
Naissance d’une discipline ; juridicisation forte ; émergence de standards et de la \emph{chain of custody}. \\
\hline
2010--2020 (Ère du Big Data et du Cloud) &
Explosion des volumes de données ; algorithmisation des procédures ; dominance des infrastructures cloud et des méthodes computationnelles. \\
\hline
\end{tabular}
\end{table}

\subsection*{b) Calcul des vecteurs de dominance}
\[
\vec R_t = (\alpha_T,\alpha_J,\alpha_S,\alpha_P), \qquad \sum_i \alpha_i = 1, \qquad \alpha_i = \frac{s_i}{\sum_j s_j}.
\]

\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{6pt}
\begin{tabular}{|c|p{8cm}|c|c|}
\hline
\textbf{Axe} & \textbf{Indicateurs principaux} & \textbf{1990--2000 ($s_i$)} & \textbf{2010--2020 ($s_i$)} \\
\hline
Technologie (T) & Internet, PC, honeypots / cloud, IA, blockchain & 2 & 9 \\
\hline
Juridique (J) & CFAA, IOCE, chain of custody / RGPD, cadres cloud & 8 & 2 \\
\hline
Social (S) & Fracture numérique / culture crypto, dark web & 1 & 4 \\
\hline
Professionnel (P) & Standardisation, ISO, forensics / data science & 6 & 5 \\
\hline
\textbf{Somme $\sum s_i$} & & \textbf{17} & \textbf{20} \\
\hline
\end{tabular}
\end{table}

\[
\vec R_{1990-2000} = (0.12, 0.47, 0.06, 0.35),
\quad
\vec R_{2010-2020} = (0.45, 0.10, 0.20, 0.25)
\]
\subsection*{c) Discontinuités épistémologiques (Foucault)}
\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.25}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{|c|p{3.5cm}|p{3.5cm}|p{5cm}|}
\hline
\textbf{Dimension} & \textbf{1990--2000} & \textbf{2010--2020} & \textbf{Rupture observée} \\
\hline
Autorité du vrai & Expert, tribunal & Algorithme, corrélation & Passage du sujet expert à l’autorité computationnelle \\
\hline
Objet du savoir & Document, log & Dataset massif, blockchain & Mutation de la trace en donnée analysable \\
\hline
Procédure de véridiction & Chaîne de custody, norme ISO & Calcul automatisé, machine learning & De la preuve normative à la preuve statistique \\
\hline
Régime de pouvoir & Régulation disciplinaire & Pouvoir infrastructural & Déplacement vers le pouvoir technique \\
\hline
\end{tabular}
\end{table}

\subsection*{d) Explication sociotechnique des ruptures}
\[
\vec R_{t+1}=F(\vec R_t,\Delta Tech_t,\Delta Legal_t,I_t)
\]
où \(\Delta Tech_t\) = progrès techniques (cloud, IA),
\(\Delta Legal_t\) = retard juridique,
\(I_t\) = incidents critiques (Silk Road, Panama Papers).  
L’augmentation de \(\alpha_T\) et la diminution de \(\alpha_J\) traduisent la
technicisation du régime de vérité.

\subsubsection*{e) Nature de la transition}
Transition progressive dans le temps, mais révolutionnaire dans sa nature :
\[
\lim_{N\to\infty}\frac{\text{Analyse humaine}}{\text{Analyse algorithmique}} = 0.
\]

\subsection*{2. Étude de cas : \textit{Silk Road} (2013)}

\subsection*{a) Contexte factuel}
Marché noir sur Tor, paiement en Bitcoin, saisie de 144\,000 BTC,
enquête FBI, analyse blockchain forensique pionnière.

\subsection*{b) Formation discursive (Foucault)}
\begin{itemize}[nosep]
\item Objets : anonymat, traçabilité, cryptomonnaie.
\item Sujets autorisés : analystes blockchain, FBI, data scientists.
\item Règles d’énonciation : validité fondée sur la signature cryptographique.
\item Champ d’énonciation : espace techno-juridique global.
\item Condition de vérité : traduction de la preuve algorithmique en preuve légale.
\end{itemize}

\subsection*{c) Ce qui est dicible et pensable}
\begin{itemize}[nosep]
\item Dicible : « La blockchain permet la traçabilité totale des transactions. »
\item Pensable : corrélation d’identités pseudonymes.
\item Inimaginable avant 2010 : recevabilité juridique d’une preuve purement algorithmique.
\end{itemize}

\subsection*{d) Cartographie du régime de vérité (La \emph{Stack})}
\begin{figure}[H]
\centering
\resizebox{0.9\textwidth}{!}{%
\begin{tikzpicture}[node distance=8mm, every node/.style={font=\small, align=center, rounded corners, draw=black!60, inner sep=6pt}, >={Stealth[round]}]
  \tikzset{layer/.style={minimum width=10.5cm, minimum height=1.2cm, fill=white}}
  \node[layer, fill=blue!15] (blockchain) {Couche 1 : \textbf{Blockchain}\\Transactions, clés publiques, hashes (preuve cryptographique)};
  \node[layer, fill=green!15, below=of blockchain] (tor) {Couche 2 : \textbf{Réseau Tor}\\Relais anonymes, métadonnées de connexion};
  \node[layer, fill=red!15, below=of tor] (app) {Couche 3 : \textbf{Application Silk Road}\\Marché noir, pseudonymes, messagerie chiffrée};
  \node[layer, fill=orange!20, below=of app] (inst) {Couche 4 : \textbf{Institutions}\\FBI, DOJ, tribunaux -- traduction judiciaire de la preuve};
  \node[layer, fill=gray!20, below=of inst] (media) {Couche 5 : \textbf{Opinion publique / médias}\\Discours de légitimation et débats sur la surveillance};

  \draw[->, thick] (blockchain) -- (tor);
  \draw[->, thick] (tor) -- (app);
  \draw[->, thick] (app) -- (inst);
  \draw[->, thick] (inst) -- (media);

  \node[draw=none, right=6mm of tor, font=\footnotesize, align=left] (legend)
  {Flux vertical de véridiction :\\
  inscription $\rightarrow$ corrélation $\rightarrow$ traduction judiciaire $\rightarrow$ légitimation sociale};
  \draw[->, thick, gray!70] (legend.west) to[out=180,in=0] (tor.east);
\end{tikzpicture}}
\caption{Cartographie multi-couches du régime de vérité dans l'affaire \textit{Silk Road}.}
\end{figure}


\subsection*{e)Synthèse comparative}\
La preuve algorithmique remplace la preuve documentaire : la vérité circule
du code vers le droit par des traductions successives.
Le pouvoir de véridiction passe des institutions à l’infrastructure technique.
L’investigateur devient opérateur de modèles ; la \emph{preuve computationnelle}
devient paradigmatique.


\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{|c|c|c|p{5cm}|}
\hline
\textbf{Élément} & \textbf{1990--2000} & \textbf{2010--2020 (Silk Road)} & \textbf{Discontinuité} \\
\hline
Régime de vérité & Juridico-professionnel & Computationnel & Mutation d'épistémè \\
\hline
Vecteur $\vec R$ & (0.12, 0.47, 0.06, 0.35) & (0.45, 0.10, 0.20, 0.25) & $\uparrow \alpha_T$, $\downarrow \alpha_J$ \\
\hline
Preuve paradigmatique & Chaîne de custody & Blockchain / Big Data & De la norme à la donnée \\
\hline
Autorité épistémique & Expert / Tribunal & Algorithme / Analyste data & Déplacement du pouvoir de validation \\
\hline
Type de transition & Progressive & Révolutionnaire & Changement d’échelle computationnel \\
\hline
\end{tabular}
\end{table}
\section*{Partie 3 : Investigation Historique Appliquée}

\subsection*{6. Reconstruction Archéologique d'Investigation}

\textbf{Cas étudié :} \textit{Affaire Kevin Mitnick (1995)} \\[0.3em]
Cette affaire illustre la transition entre un régime de vérité \textbf{technique} et un régime \textbf{juridico-professionnel}. 
Le pouvoir de véridiction se déplace du technicien individuel vers l’expert institutionnalisé, marquant ainsi la naissance d’une véritable épistémè de la traçabilité numérique.

\[
\vec{R}_{1995} = (\alpha_T = 0.35, \, \alpha_J = 0.40, \, \alpha_S = 0.15, \, \alpha_P = 0.10)
\]

\noindent\textbf{Les caractéristiques principales :}
\begin{itemize}
    \item \textbf{La nature du régime :} juridico-technique, centré sur la chaîne de custody.
    \item \textbf{L'Epistémè dominante :} la vérité découle de la traçabilité et de la préservation des preuves.
    \item \textbf{Les acteurs clés :} enquêteurs fédéraux, experts indépendants, autorités judiciaires.
\end{itemize}

\noindent\textbf{Les méthodes et outils de l'époque :}
\begin{itemize}
    \item L'analyse \textit{manuelle} des journaux système (logs UNIX, adresses IP, timestamps).
    \item L'utilisation d’outils rudimentaires : \texttt{WHOIS}, \texttt{traceroute}, \texttt{netstat}.
    \item La corrélation temporelle humaine entre événements et connexions.
    \item La conservation de preuves sur supports physiques (disquettes, impressions papier).
    \item La légitimité de la preuve fondée sur la \textit{réputation de l’expert}.
\end{itemize}

\noindent\textbf{La reconstruction contemporaine :}
\begin{itemize}
    \item l'emploi de plateformes \textbf{SIEM} et d’intelligences artificielles de corrélation automatisée.
    \item La vérification d’intégrité par \textbf{empreintes cryptographiques} (hash, blockchain).
    \item l'analyse de graphes comportementaux pour la corrélation d’identités.
    \item l'archivage des preuves dans des \textbf{registres immuables et distribués}.
    \item l'attribution algorithmique fiable et reproductible.
\end{itemize}

\begin{center}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Dimension} & \textbf{1995} & \textbf{2025} \\
\hline
Collecte & Extraction manuelle & Collecte automatisée \\
Analyse & Corrélation humaine & Corrélation algorithmique \\
Autorité de preuve & Expertise individuelle & Légitimité computationnelle \\
Régime de vérité & Juridico-technique & Computationnel-algorithmique \\
\hline
\end{tabular}
\end{center}

\begin{quote}
La vérité ne se dit plus, elle se calcule.  
La transition de la preuve technique à la preuve algorithmique illustre le passage du \textit{sujet expert} au \textit{système calculateur} comme producteur de vérité.
\end{quote}

% ------------------------------------------------------------

\subsection*{7. Projet de Recherche Archéologique}

\noindent\textbf{Problématique identifiée :}  
L’archéologie de l’investigation numérique présente un \textit{trou historique} notable : la période 1980–1990, où le hacking artisanal s’est progressivement institutionnalisé en expertise technique reconnue.

\noindent\textbf{Hypothèse de recherche :}
\begin{quote}
La formalisation du cadre légal de la preuve numérique n’a pas émergé des avancées technologiques, mais des scandales médiatiques qui ont rendu la vérité technique socialement dicible.
\end{quote}

\noindent\textbf{Méthodologie archéologique :}
\begin{itemize}
    \item l'analyse des \textbf{conditions discursives de possibilité} de la preuve numérique.
    \item l'étude des \textbf{textes fondateurs} : RFC 1087 (\textit{Ethics and the Internet}), Computer Fraud and Abuse Act (1986), récits médiatiques de l’affaire des 414s.
    \item Cartographie du \textbf{réseau d’acteurs} : hackers, journalistes, législateurs, ingénieurs.
    \item L'application du cadre foucaldien : repérage des formations discursives et des pratiques de légitimation de la vérité.
\end{itemize}

\noindent\textbf{Résultats attendus :}
\begin{itemize}
    \item La mise en évidence d’un \textbf{proto-régime de vérité hybride}, entre artisanat technique et institutionnalisation juridique.
    \item L'analyse du rôle performatif du \textbf{discours médiatique} dans la création du droit numérique.
    \item La proposition d’un modèle dynamique reliant \textbf{visibilité sociale} et \textbf{formalisation juridique}.
\end{itemize}

\begin{quote}
La société n’a pas légitimé la preuve technique parce qu’elle la comprenait, mais parce qu’elle la craignait.  
Cette peur médiatisée a constitué la matrice de la juridicisation du numérique.
\end{quote}

% ------------------------------------------------------------

\subsection*{8. Analyse Prospective des Régimes Futurs (2030–2050)}

\noindent\textbf{Scénario envisagé :} \textit{Régime neuro-digital} \\[0.3em]
Ce régime émergerait de la convergence entre \textbf{IA cognitive}, \textbf{biométrie neuronale} et \textbf{interfaces cerveau-machine}.  
La trace numérique deviendrait \textit{interne au sujet}, incorporée à son activité cérébrale.

\noindent\textbf{Les conditions de possibilité :}
\begin{itemize}
    \item La captation et interprétation directe des signaux neuronaux comme éléments de preuve.
    \item la validation des souvenirs numériques par empreintes cérébrales certifiées.
    \item le déplacement du sujet de savoir : du corps technique à l’esprit numérisé.
\end{itemize}

\noindent\textbf{Méthodologie d’investigation adaptée :}
\begin{itemize}
    \item \textbf{Neuro-forensique} : analyse des patterns cérébraux liés aux actes numériques.
    \item \textbf{La blockchain cognitive} : enregistrement sécurisé des flux neuronaux.
    \item \textbf{Audit éthique par IA} : supervision non intrusive garantissant la confidentialité mentale.
\end{itemize}

\noindent\textbf{Défis éthiques et épistémologiques :}
\begin{itemize}
    \item \textbf{Opposabilité :} comment vérifier une preuve sans violer la vie mentale ?
    \item \textbf{Authenticité :} comment distinguer un souvenir réel d’une reconstruction neuronale ?
    \item \textbf{Confiance :} la machine peut-elle devenir sujet de vérité ?
\end{itemize}

\begin{quote}
Le régime neuro-digital représenterait une discontinuité radicale.  
Il verrait l’abandon du \textit{sujet parlant} au profit du \textit{sujet calculant et observé}.  
La vérité ne serait plus énoncée, mais extraite du signal cérébral, inaugurant une ère post-humaine de l’investigation.
\end{quote}


\section*{Partie 2 : Modélisation Mathématique et Prospective}

\vspace{0.5cm}

% --------------------
\subsection*{3. Modélisation de l'évolution des régimes (question par question)}

\paragraph{Modèle général} On représente chaque régime par un vecteur de dominance :
\[
\vec{R}_t = (\alpha_T,\alpha_J,\alpha_S,\alpha_P),
\qquad \sum \alpha_i = 1.
\]
La transition est modélisée par une fonction paramétrique :
\[
\vec{R}_{t+1} = F(\vec{R}_t,\Delta Tech_t,\Delta Legal_t,I_t,\theta),
\]
où $\Delta Tech_t$ et $\Delta Legal_t$ sont des variables numériques représentant l'amplitude du changement technique et juridique entre $t$ et $t+1$, $I_t$ est un indicateur d'incidents critiques, et $\theta$ l'ensemble des paramètres du modèle.

\paragraph{Approche discrétisée (chaîne de Markov conditionnée)} Pour passer de la représentation vectorielle à une distribution de probabilités de \emph{changement d'état}, on discrétise l'espace des régimes en états finis :
\[
\mathcal{S}=\{T,J,S,C\}
\]
(Technique, Juridique, Standardisé, Computationnel). On modélise la probabilité conditionnelle
\[
P\big( R_{t+1}=j \mid R_t=i, X_t\big) \quad (j\in\mathcal{S})
\]
où \(X_t=(\Delta Tech_t,\Delta Legal_t,I_t)\). Une paramétrisation usuelle est la \emph{multinomial logit} (softmax) :
\[
\text{score}_j = \beta_{j0} + \beta_{j1}\,\Delta Tech_t + \beta_{j2}\,\Delta Legal_t + \beta_{j3}\,I_t,
\]
\[
P(R_{t+1}=j\mid R_t=i,X_t) = \frac{\exp(\text{score}_j)}{\sum_{k\in\mathcal{S}}\exp(\text{score}_k)}.
\]

% ---------- FIGURE 1: espace d'états ----------
\begin{figure}[H]
\centering
\begin{tikzpicture}[node distance=2.2cm, auto, thick]
\tikzstyle{st}=[circle, draw=black!70, fill=black!5, minimum size=9mm, align=center]
\node[st] (T) {Technique\\(T)};
\node[st, right of=T, xshift=1.3cm] (J) {Juridique\\(J)};
\node[st, below of=T, yshift=-1.2cm] (S) {Standardisé\\(S)};
\node[st, right of=S, xshift=1.3cm] (C) {Computationnel\\(C)};

\draw[->] (T) to[bend left=20] (J);
\draw[->] (J) to[bend left=20] (C);
\draw[->] (C) to[bend left=20] (S);
\draw[->] (S) to[bend left=20] (T);
\draw[->, dashed] (T) -- (C);
\draw[->, dashed] (J) -- (S);
\end{tikzpicture}
\caption{Espace d'états discretisé (exemple) : quatre régimes et transitions possibles.}
\end{figure}

% ---------- FIGURE 2: bloc F ----------
\begin{figure}[H]
\centering
\begin{tikzpicture}[node distance=1.8cm, auto, thick]
\tikzstyle{blk}=[rectangle, draw=black!70, fill=gray!10, rounded corners, minimum width=5cm, minimum height=1.2cm, align=center]
\node (in1) { $\Delta Tech_t$};
\node[right of=in1, xshift=2.2cm] (in2) { $\Delta Legal_t$};
\node[right of=in2, xshift=2.2cm] (in3) { $I_t$};
\node[blk, below of=in2, yshift=-0.2cm] (F) {$F(\cdot;\theta)$ \\ (fonction de transition)};
\node[below of=F, yshift=-0.2cm] (out) {Distribution $\;P(R_{t+1}\mid R_t,X_t)$};

\draw[->] (in1) -- (F);
\draw[->] (in2) -- (F);
\draw[->] (in3) -- (F);
\draw[->] (F) -- (out);
\end{tikzpicture}
\caption{Schéma fonctionnel : entrée (features) vers la fonction de transition $F$ puis distribution de sortie.}
\end{figure}

% ---------- FIGURE 3: histogramme probabilités (exemple numérique) ----------
\begin{figure}[H]
\centering
\begin{tikzpicture}[y=1.2cm, x=3cm]
% bars base y=0
\filldraw[fill=blue!50] (0,0) rectangle (0.6,0.2290);
\filldraw[fill=green!60!black] (1,0) rectangle (1.6,0.4611);
\filldraw[fill=orange!70] (2,0) rectangle (2.6,0.1334);
\filldraw[fill=purple!60] (3,0) rectangle (3.6,0.1765);
\node at (0.3,-0.06) {\small T};
\node at (1.3,-0.06) {\small J};
\node at (2.3,-0.06) {\small S};
\node at (3.3,-0.06) {\small C};
\node[below=1.1cm] at (1.8,0) {\small Probabilités de transition (exemple numérique) };
% labels
\node[above] at (0.3,0.2290) {\scriptsize 22.896\%};
\node[above] at (1.3,0.4611) {\scriptsize 46.107\%};
\node[above] at (2.3,0.1334) {\scriptsize 13.343\%};
\node[above] at (3.3,0.1765) {\scriptsize 17.654\%};
\end{tikzpicture}
\caption{Histogramme des probabilités de transition calculées dans l'exemple numérique (softmax).}
\end{figure}

\vspace{0.4cm}
\paragraph{Calcul numérique d'exemple }\  
On illustre la formule avec un jeu de paramètres \(\beta\) et des valeurs de \(X_t\).

\textbf{Paramètres choisis} (exemple pédagogique) :
\[
\begin{array}{lcl}
\text{Pour }j=T: & \beta_{T0}=1.0,\; \beta_{T1}=2.0,\; \beta_{T2}=-1.0,\; \beta_{T3}=-0.5,\\[2pt]
\text{Pour }j=J: & \beta_{J0}=0.5,\; \beta_{J1}=0.5,\; \beta_{J2}=1.5,\; \beta_{J3}=0.8,\\[2pt]
\text{Pour }j=S: & \beta_{S0}=0.2,\; \beta_{S1}=0.3,\; \beta_{S2}=0.7,\; \beta_{S3}=0.1,\\[2pt]
\text{Pour }j=C: & \beta_{C0}=-0.5,\; \beta_{C1}=1.0,\; \beta_{C2}=0.2,\; \beta_{C3}=0.9.
\end{array}
\]

\textbf{Valeurs des variables}  :
\[
\Delta Tech_t = 0.4,\qquad \Delta Legal_t = 0.2,\qquad I_t = 1.
\]

\noindent\textbf{Étape 1 — calcul des scores linéaires}
\[
\begin{aligned}
s_T &= \beta_{T0} + \beta_{T1}\cdot 0.4 + \beta_{T2}\cdot 0.2 + \beta_{T3}\cdot 1 \\
    &= 1.0 + 2.0\times 0.4 + (-1.0)\times 0.2 + (-0.5)\times 1 \\
    &= 1.0 + 0.8 - 0.2 - 0.5 = 1.10,\\[6pt]
s_J &= 0.5 + 0.5\times 0.4 + 1.5\times 0.2 + 0.8\times 1 \\
    &= 0.5 + 0.2 + 0.3 + 0.8 = 1.80,\\[6pt]
s_S &= 0.2 + 0.3\times 0.4 + 0.7\times 0.2 + 0.1\times 1 \\
    &= 0.2 + 0.12 + 0.14 + 0.1 = 0.56,\\[6pt]
s_C &= -0.5 + 1.0\times 0.4 + 0.2\times 0.2 + 0.9\times 1 \\
    &= -0.5 + 0.4 + 0.04 + 0.9 = 0.84.
\end{aligned}
\]

\noindent\textbf{Étape 2 — exponentiation (softmax numerator)} \\
Nous calculons $\exp(s_j)$ pour chaque score (valeurs arrondies à 10 decimales) :
\[
\begin{aligned}
\exp(s_T) &= e^{1.10} \approx 3.0041660239,\\
\exp(s_J) &= e^{1.80} \approx 6.0496474644,\\
\exp(s_S) &= e^{0.56} \approx 1.7506725003,\\
\exp(s_C) &= e^{0.84} \approx 2.3163669768.
\end{aligned}
\]

\noindent\textbf{Étape 3 — normalisation} \\
Somme des exponentielles :
\[
Z = \sum_{j\in\{T,J,S,C\}} \exp(s_j) \approx 3.0041660239 + 6.0496474644 + 1.7506725003 + 2.3163669768 \approx 13.1208529654.
\]

\noindent\textbf{Étape 4 — probabilités finales} \\
\[
\begin{aligned}
P_T &= \frac{3.0041660239}{13.1208529654} \approx 0.2289611835 \;(\approx 22.896\%),\\[3pt]
P_J &= \frac{6.0496474644}{13.1208529654} \approx 0.4610712032 \;(\approx 46.107\%),\\[3pt]
P_S &= \frac{1.7506725003}{13.1208529654} \approx 0.1334267296 \;(\approx 13.343\%),\\[3pt]
P_C &= \frac{2.3163669768}{13.1208529654} \approx 0.1765408837 \;(\approx 17.654\%).
\end{aligned}
\]

\noindent Ces nombres illustrent comment, pour les valeurs choisies, l'état \textit{Juridique} (J) a la probabilité la plus élevée de devenir le régime suivant.

\vspace{0.5cm}
\paragraph{Remarques méthodologiques}
\begin{itemize}
  \item Les coefficients $\beta$ sont estimables par maximum de vraisemblance sur une série historique de transitions observées (log-likelihood multinomial).  
  \item Dans la pratique, on conditionne souvent la probabilité sur l'état courant $R_t$ (effet d'auto-corrélation) et on ajoute des variables d'interaction (par ex. $I_t\times \Delta Tech_t$).  
  \item Validation : cross-validation temporelle, calibration probabiliste (reliability diagrams) et tests de robustesse aux incidents extrêmes.
\end{itemize}


\subsection*{4. Vérification de l'accélération technologique }

\paragraph{Loi proposée} On teste la loi empirique :
\[
\Delta t_{n+1} = k\cdot \Delta t_n,\qquad 0<k<1.
\]
Pratiquement, on dispose d'une suite de dates $t_0<t_1<\dots<t_N$ où des ruptures de régime sont datées ; on forme $\Delta t_n = t_n - t_{n-1}$ pour $n=1,\dots,N$.

\paragraph{Estimation de \(k\)} en transformant logarithmiquement :
\[
\ln(\Delta t_{n+1}) = \ln k + \ln(\Delta t_n).
\]
Donc une régression linéaire simple (sans intercept centré si on préfère) sur les paires $(\ln\Delta t_n,\ln\Delta t_{n+1})$ donne une estimation de $\ln k$ ; on en déduit $k=\exp(\widehat{\ln k})$.

\paragraph{Procédure pratique}
\begin{enumerate}
  \item Calculer $\Delta t_n$ à partir des dates historiques.  
  \item Prendre logarithme : $y_n=\ln(\Delta t_{n+1})$, $x_n=\ln(\Delta t_n)$.  
  \item Estimer par moindres carrés : $y_n = a + b x_n + \varepsilon_n$. Le coefficient $b$ proche de $1$ indique une relation multiplicative directe ; ici on s'attend à $b\approx 1$ et $a=\ln k$.  
  \item Tester $b=1$ et significativité de $a$ (p-valeur).  
  \item Robustesse : utiliser un estimateur robuste (Theil–Sen) si outliers (événements extrêmes).
\end{enumerate}

\paragraph{Remarque de mise en garde} La «loi» est empirique : la constance de \(k\) doit être testée par période ; un seul \(k\) global est rarement réaliste sur un siècle.

% --------------------
\subsection*{5. Analyse du Trilemme CRO (question 5)}

\paragraph{Définition} Pour un système \(S\) on considère trois scores normalisés :
\[
C(S)\in[0,1]\ \text{(Confidentialité)},\quad R(S)\in[0,1]\ \text{(Fiabilité)},\quad O(S)\in[0,1]\ \text{(Opposabilité)}.
\]
Le trilemme affirme qu'il est impossible d'atteindre simultanément \(C=R=O=1\).

\paragraph{Représentation géométrique} On place ces trois dimensions aux sommets d'un triangle de décisions ; les régimes historiques se projettent à l'intérieur.

\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=3]
% triangle
\coordinate (A) at (0,0);
\coordinate (B) at (1,0);
\coordinate (C) at (0.5,0.86602540378);
\draw[thick] (A)--(B)--(C)--cycle;
\node[below] at (A) {Confidentialité};
\node[below] at (B) {Opposabilité};
\node[above] at (C) {Fiabilité};
% sample points
\filldraw[black] (0.25,0.25) circle (0.8pt) node[left]{\scriptsize 1990s};
\filldraw[black] (0.53,0.37) circle (0.8pt) node[right]{\scriptsize 2010s};
\filldraw[black] (0.7,0.15) circle (0.8pt) node[right]{\scriptsize 2020s};
\end{tikzpicture}
\caption{Triangle du Trilemme CRO — positionnement indicatif de régimes historiques.}
\end{figure}

\paragraph{Analyse quantitative} Si l'on dispose de mesures \(C_n,R_n,O_n\) pour différentes époques \(n\), on peut :
\begin{itemize}
  \item tracer la trajectoire $(C_n,R_n,O_n)$ dans l'espace de décision (PCA ou coordonnées barycentriques) ;  
  \item rechercher des compromis optimaux via une optimisation multi-objectif (Pareto front).  
\end{itemize}




\end{document}
